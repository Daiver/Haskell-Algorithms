Haskell на практике-2

В этом блоге уже много написано о самом языке Haskell, и было несколько статей о его практическом применении. Сейчас я расскажу еще об одном реальном применении языка в производстве.

Я работаю в телекомах: фиксированная телефонная связь, интернет, IP-телефония. В задачи входит обработка трафика с АТС, с серверов IP-телефонии, поддержка текущего биллинга, написание софта и администрирование. Недавно в моей ораганизации было установлено новое оборудование, трафик изменился. Теперь он поставляется в двух вариантах со старых станций и с новых. Новый трафик дублировался бинарном и текстовом форматах. Бинарники нам вообще не подходят (хотя кое-кто по незнанию говорил: "А чего там? Берешь их, и они сами в биллинг заскакивают!", ага, ага, как же), текстовый же занимает на порядок больше места, но именно с ним можно было что-то сделать. Трафик со старого оборудования все еще идет, и его мы с напарником забрасываем с помощью сервисов СУБД. Можно было бы настроить эти же сервисы для трафика с новых станций, но обнаружилось несколько особенностей. Трафик на новых станциях пишется в отдельные файлы каждые 15 минут; всего таких файлов в сутки получается около 3000 (в идеале - 2976 за 31 день и 2880 за 30 дней). Каждый файл отдельно импортировать не будешь, ибо маразм. Слить в один их можно и даже нужно, благо они все текстовые, и записи о звонках расположены построчно. Ручное слияние выглядит так: выделяешь файлы только за прошлый месяц и добавляешь их в простейший скрипт слияния в командной строке. Формат имени файла фиксирован, следовательно, слияние можно автоматизировать, только придется парсить год и месяц. Линуксоиды бы использовали какой-нибудь Bash, Perl или Python, - дешево и сердито, но на Windows-машину их ради одной операции не поставишь, то же самое касается и PowerShell. А cmd - это извращение, о чем мы с вами хорошо знаем. ;) Наконец, в самом трафике тоже обнаружились сюрпризы, из-за которых даже после слияния и импорта с помощью средств СУБД требовалось много ручной SQL-работы. В общем, факторы как-то так удачно сложились в задачу для Haskell, который я в то время (апрель-май 2011) начал изучать.

Описание данных

Итак, ~ 3000 15-тиминутных файлов за месяц. На оборудовании можно изменить интервал и поставить не 15 минут, а любое другое значение: 5, 10, 30, 45... Количество файлов и их размеры, соответственно, изменятся. Пример имени файла (за 09.05.2011 09:30:00):

999992011050909300081.txt
99999                      - идентификатор, вшитый в оборудование (по понятным причинам, я его заменил на девятки)
     2011                  - год
         05                - месяц
           09              - день
             09            - часы
               30          - минуты
                 00        - секунды
                   81      - какое-то случайное число, возможно, десятые доли секунд.

Абонентов становится больше, и каждый файл уверенно растет в размере. Сейчас у нас в среднем 240 строчек на файл, но было сезонное летнее проседание, люди разъехались по отпускам и звонили меньше. За сентябрь ждем увеличение активности в полтора-два раза.

Записи о звонках разнообразны. Есть несколько разных типов записей, у которых разное количество полей. Записи типа R210 попадаются очень редко, и что они значат, мы не стали выяснять (здесь и далее данные трафика заменены случайными):

|R210|2011-06-24 21:43:53|2011-06-24 01:43:52|1|

Как нетрудно убедиться, здесь всего лишь 4 поля: идентификатор типа записи, дата начала, дата конца (формат ISO 8601/SQL) и, зачем-то, единичка. Поля разделяются вертикальной чертой, которая должна стоять и в начале записи, и в конце, так что реально полей на 1 больше, то есть, 5. Удобно считать, что поле с индексом 0 пустое, находится перед первой вертикальной чертой. Тогда отсчет значимых полей будет идти с 1.

Обычные звонки регистрируются в записях типа R200. Там уже 152 поля, причем это можно перенастроить на оборудовании: какие-то поля добавить, какие-то удалить, а у прочих, возможно, поменять формат.

|R200|99999|111111|CR,CS,AM|1|1|3022|222222|333333|||2011-06-23 11:33:58|C|2011-06-23 11:34:22|S|0|16|1||||||1|1||||||3|162|17|1|12|24|||||||||||16|0||||||192.168.1.172||192.168.1.12||||||8|8|20|20|64|64|20|0|0|OS|7777|8888|555555|666666|0|8|9||||OS|19|||30|10|42|43||||||||||||1||||||1|1|0|3||222222|||||||2|1||333333|||||||||||||||||||||||||||||||

Нас интересуют поля с индексами 7, 8, 9, 12, 14, 36, 112, 122, и в конечном результате хотелось бы отфильтровать всё ненужное, чтобы лишнего в СУБД не импортировать. Выбрав из сырых данных только нужное, получим строку:

Запись:   3022|222222|333333|2011-06-23 11:33:58|2011-06-23 11:34:22|24|222222|333333
Индексы:  7   |8     |9     |12                 |14                 |36|112   |122 

Индексы |  Объяснение
---------------------------
7       |  код города Читы
8, 112  |  исходящий номер
9, 122  |  входящий номер
12      |  дата и время начала разговора
14      |  дата и время конца разговора
36      |  длительность разговора в секундах

Все остальные поля особо не нужны. Некоторые, как вы видите, вообще пустые, а смысл прочих - неизвестен. Разве что IP-адреса (изменены) принадлежат двум платам в инфраструктуре телефонной сети, между которыми пойдет RTP-трафик.

Записи в трафике идут плотно, строка за строкой. Возможно, есть какие-то другие типы записей, но они нам не интересны. Для тарификации достаточно только записей типа R200. Однако, во время визуального исследования трафика выяснилось еще одно интересное обстоятельство. Иногда попадались звонки с одного номера, начавшиеся в одно и то же время, но длительность которых была разная. Сначала, в условиях неполной информации я подумал, что это какой-то глюк, ведь не может же человек звонить параллельно с одного и того же номера. Потом стала просматриваться закономерность, и, наконец, я понял, в чем дело. Вот пример таких записей на один номер телефона, все лишние поля я выкинул для наглядности:

|3022|222222|333333|2011-05-23 13:07:54|2011-05-23 13:37:54|1800|
|3022|222222|333333|2011-05-23 13:07:54|2011-05-23 13:59:40|3106|

|3022|444444|555555|2011-05-23 14:53:52|2011-05-23 15:23:52|1800|
|3022|444444|555555|2011-05-23 14:53:52|2011-05-23 15:53:52|3600|
|3022|444444|555555|2011-05-23 14:53:52|2011-05-23 16:00:50|4018|

|3022|666666|777777|2011-05-23 19:15:55|2011-05-23 19:45:54|1800|
|3022|666666|777777|2011-05-23 19:15:55|2011-05-23 20:15:54|3600|
|3022|666666|777777|2011-05-23 19:15:55|2011-05-23 20:45:54|5400|
|3022|666666|777777|2011-05-23 19:15:55|2011-05-23 20:47:17|5483|

Вам-то сейчас видно, в чем соль, а тогда мне эти записи среди тысяч им подобных, среди кучи лишних полей, букв и цифр, найти было нелегко. В общем, это было или везение, или интуиция, или волшебство. :) А разгадка проста: оборудование каждые полчаса (1800 секунд) отмечает "веху разговора" на случай, если что-то произойдет. Даже если последние 29 минут разговора были почему-то утеряны, весь предыдущий трехчасовой дискурс был многажды зафиксирован - для пущей надежности. В последней записи будут актуальные данные. Возможно, на оборудовании длительность вех можно как-то изменить, а пока их ряд выглядит так: 1800, 3600, 5400, 7200, 9000, 10800... Здесь еще примечательно то, что записи с вехами отличаются в нескольких "неважных" полях от самых последних записей. Возможно, стоит учесть это в будущем, чтобы более качественно отфильтровывать ненужное, а пока я принял решение все записи с длительностью из этого ряда просто выбрасывать. Теоретически, здесь какой-то малый процент нормальных звонков будет утерян, но для этого нужно, чтобы человек разговаривал полчаса в точности до секунды. Вероятность этого очень маленькая, а объемы у нас не такие значительные, чтобы закон больших чисел как-то повлиял на выборку.

Описание программ

Всего я создал четыре программы, которые сливали нужные файлы и отфильтровывали полезную информацию. Первоначально это были merger и parser - две программы, написанные по отдельности и соединенные в третью, mergerparser. Все они работали крайне медленно и потребляли много памяти, хотя с задачами справлялись. Данные одного месяца (3000 файлов по 240 строк = 720000 строк) они могли обрабатывать, минимум, 10 минут с отжиранием 500 мб памяти (а то и больше, ведь работал своп). Очень, очень страшный результат. И хотя задачу надо было выполнять 1 раз в месяц, мой напарник презрительно морщил нос на Haskell. Правда, Haskell тут ну совершенно ни при чем; это я допустил в программах ряд типичных ошибок начинающего функциональщика, из-за чего кривые алгоритмы работали из рук вон плохо. Но работали! Даже больше: программы (а самая большая из них, mergerparser, занимает всего лишь 150 полезных строк) можно было настроить из командной строки. Вот какие функции были доступны:

1. Работа в режиме без параметров. Поля берутся по умолчанию, берутся файлы прошлого месяца.
2. Работа в режиме с параметрами:
- Fields [<список индексов полей>] - какие поля взять (parser Fields [1, 24, 55]);
- (yyyy, mm) - какой месяц обрабатывать (merger (2011, 5));
-W - не закрывать окно после отработки (от слова "wait").
3. Получались три файла:
- yyyy.mm.txt - в него сливались все файлы с сырым трафиком за этот месяц;
- processed.txt - файл только с нужными полями за этот месяц;
- yyyy.mm.txt.log - файл-лог, где перечисляются задействованные сырые файлы и пишется сводная информация (количество строк, файлов, диапазон дат).
4. Программы выводят на экран статистику и примеры обработанного трафика.

Пару-тройку раз мы пользовались, чем было, но потом я, конечно, переписал программу с нуля. Уж очень в старом коде было много кривого кода, ненужных велосипедов, глупых алгоритмов и странных решений. В итоге четвертая программа, NgnParser, при той же функциональности и на том же наборе данных работает не 10 минут, а 10 секунд, потребляя всего лишь 10 мб памяти. По скорости разница составляет почти два порядка и, как минимум, один - по памяти! Что можно было такого намутить, чтобы _так_ замедлить программу? Полагаю, есть люди, наступившие на те же грабли, что и я, которые поверили скорее в тормознутость языка, чем в свои кривые руки, - недаром в Интернете так много воплей по поводу и без повода... Haskell - замечательный язык. Мне было легко писать эти программы. На каждую я тратил не более двух рабочих дней. И всякий раз получал море удовольствия. Не представляю, сколько было бы мучений, если бы то же самое я делал на Сях.

Первое поколение

